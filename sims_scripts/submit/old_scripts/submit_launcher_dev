#!/bin/bash
#SBATCH -J 1k_events_iS3D_adaptive 
#SBATCH -p skx-dev        # Queue (partition) name
#SBATCH -N 2                 # the number of nodes
#SBATCH --ntasks-per-node 48 #set this equal to number of cores on node
#SBATCH -t 01:00:00          # Run time (hh:mm:ss)   walltime for each node separately
#SBATCH --mail-user=everett.165@osu.edu
#SBATCH --mail-type=all      # Send email at begin and end of job
#SBATCH -A TG-PHY180035      # Allocation name (Large allocation)
#SBATCH -o slurm/out-%j
#SBATCH -e slurm/err-%j

module load intel/18.0.2 cmake/3.7.1 gsl boost hdf5 eigen impi python3 launcher

source ../prepare_compilation_stampede2_2.sh
module list

inputdir=../input-config/ #where the input files for all modules are located. remember to run ./generate_module_input_files.py first 
job=$SLURM_JOB_ID
# create taskfarmer tasks
rm -f tasks # remove old tasks!!! important!!!

#total number of hydro events will be (ntasks * nevents) assuming job remains within walltime 
ntasks=10
for task in $(seq -f "%05g" $ntasks); do
    echo run-events-final --nevents 1 --logfile $SCRATCH/$job/$task.log --tmpdir=$SCRATCH/ --startdir=$inputdir $job/$task.dat >> tasks
done

export LAUNCHER_PLUGIN_DIR=$LAUNCHER_DIR/plugins
export LAUNCHER_RMI=SLURM
export LAUNCHER_JOB_FILE=tasks

$LAUNCHER_DIR/paramrun


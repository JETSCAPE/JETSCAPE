#!/usr/bin/env python3

import argparse
from contextlib import contextmanager
import datetime
from itertools import chain, groupby, repeat
import logging
import math
import os
import pickle
import signal
import subprocess
import sys
import tempfile
import numpy as np



# fully specify numeric data types, including endianness and size, to
# ensure consistency across all machines
float_t = '<f8'
int_t = '<i8'
complex_t = '<c16'

# species (name, ID) for identified particle observables
species = [
	('pion', 211),
	('kaon', 321),
	('proton', 2212),
	('Lambda', 3122),
	('Sigma0', 3212),
	('Xi', 3312),
	('Omega', 3334),
	('phi', 333),
]
pi_K_p = [
	('pion', 211),
	('kaon', 321),
	('proton', 2212),
]

NpT = 10
Nharmonic = 8
Nharmonic_diff = 5

# Particle datatype
particle_dtype = [
				('ID', int),
				('charge', int),
				('pT', float),
				('ET', float),
				('mT', float),
				('phi', float),
				('y', float),
				('eta', float),
			]

# results "array" (one element)
# to be overwritten for each event
result_dtype=[
('initial_entropy', float_t, 1),
('impact_parameter', float_t, 1),
('npart', float_t, 1),
('ALICE',
	[
		# 1) dNch/deta, eta[-0.5, 0.5], charged
		('dNch_deta', float_t, 1),
		# 2) dET/deta, eta[-0.6, 0.6]
		('dET_deta', float_t, 1),
		# 3.1) The Tmunu observables, eta[-0.6, 0.6]
		('Tmunu', float_t, 10),
		# 3.2) The Tmunu observables, eta[-0.5, 0.5], charged
		('Tmunu_chg', float_t, 10),
		# 4.1) identified particle yield
		('dN_dy', 	[(name, float_t, 1) for (name,_) in species], 1),
		# 4.2) identified particle <pT>
		('mean_pT', [(name, float_t, 1) for (name,_) in species], 1),
		# 5.1) pT fluct, pT[0.15, 2], eta[-0.8, 0.8], charged
		('pT_fluct_chg', [	('N', int_t, 1),
							('sum_pT', float_t, 1),
							('sum_pT2', float_t, 1)], 1),
		# 5.2) pT fluct, pT[0.15, 2], eta[-0.8, 0.8], pi, K, p
		('pT_fluct_pid', [	(name, [	('N', int_t, 1),
										('sum_pT', float_t, 1),
										('sum_pT2', float_t, 1)], 1	)
							  for (name,_) in pi_K_p	], 1),
		# 6) Q vector, pT[0.2, 5.0], eta [-0.8, 0.8], charged
		('flow', [	('N', int_t, 1),
					('Qn', complex_t, Nharmonic)], 1),
		# 7) Q vector, diff-flow eta[-0.8, 0.8], pi, K, p
		# It uses #6 as its reference Q vector
		('d_flow_chg', [('N', int_t, NpT),
						('Qn', complex_t, [NpT, Nharmonic_diff])], 1),
		('d_flow_pid', [(name, [('N', int_t, NpT),
								('Qn', complex_t, [NpT, Nharmonic_diff])], 1)
						for (name,_) in pi_K_p	], 1),
	], 4)
]

# A function that calculates event-planes given a list of azimuthal angles
def calculate_eventplanes(phi, Nharmonic):
	V_n = {n: float_t for n in range(1,Nharmonic+1)}
	Psi_n =  {n: float_t for n in range(1,Nharmonic+1)}
	N = phi.size
	Npairs = N*(N-1.)
	for n in range(1, Nharmonic+1):
		Qx = np.cos(n*phi).sum()
		Qy = np.sin(n*phi).sum()
		V_n[n] = np.sqrt( (Qx**2 + Qy**2 - N)/pairs )
		Psi_n[n] = np.arctan2(Qy, Qx)/n
	return V_n, Psi_n

# A function that calculates double differential q-vectors
# given a list of pT-bins and a list of eta-bins
# for example:
#	 pTbins = np.array([[0,1],[1,2],[2,3]])
#	etabins = np.array([[-3,-1],[-1,1],[1,3]])
def calc_Qvector(pT, phi, eta, pTbins, etabins, Nharmonic):
	shape = [pTbins.shape[0], etabins.shape[0]]
	# calculate Qn
	results = {'N': np.zeros(shape, dtype=int_t),
			   'Qn': np.zeros([*shape, Nharmonic], dtype=complex_t)}
	for i, (pT_l, pT_h) in enumerate(pTbins):
		pT_cut = (pT_l<=pT) & (pT<=pT_h)
		for j, (eta_l, eta_h) in enumerate(etabins):
			eta_cut = (eta_l<=eta) & (eta<=eta_h)
			# apply cut
			sub_phi = phi[pT_cut & eta_cut]
			results['N'][i,j] = sub_phi.size

			results['Qn'][i,j] = np.array([ np.exp(1j*n*sub_phi).sum()
										   for n in range(1, 1+Nharmonic) ])
	return results


# A function that calculates J-F's magic Tmunu observables (~ int dV T^{mu,nu} )
# Given a y-cut = [-1,1], and a pT-cut = [.2,5.0] e.g.
def calc_Tmunu(ET, y, pT, phi, ycut, pTcut):
	Tmunu = np.zeros(10)

	cut = (pTcut[0]<pT) & (pT<pTcut[1]) & (ycut[0]<y) & (y<ycut[1])
	# apply cut
	y = y[cut]
	ET = ET[cut]
	phi = phi[cut]
	pT = pT[cut]

	Pmu = np.array([ET * np.cosh(y), pT * np.cos(phi),
					pT * np.sin(phi), ET * np.sinh(y) ])

	# T0nu
	for i in range(4):
		Tmunu[i] = Pmu[i].sum()
	# Tij
	n = 4
	for i in range(1,4):
		for j in range(i, 4):
			Tmunu[n] = (Pmu[i]*Pmu[j]/Pmu[0]).sum()
			n += 1

	return Tmunu

# loop over the datatype iteratively and print every thing for a check
def dprint(data, dtype, prefix):
	for (name, sub_dtype, _) in dtype:
		sub_prefix = prefix+"/"+name
		if type(sub_dtype) == list:
			dprint(data[name], sub_dtype, sub_prefix)
		else:
			print(sub_prefix,'=', data[name][0])


def file_len(fname):
	""" do line count """
	p = subprocess.Popen(['wc', '-l', fname], stdout=subprocess.PIPE,
											  stderr=subprocess.PIPE)
	result, err = p.communicate()
	if p.returncode != 0:
		raise IOError(err)
	return int(result.strip().split()[0])

def run_cmd(*args):
	"""
	Run and log a subprocess.

	"""
	cmd = ' '.join(args)
	logging.info('running command: %s', cmd)

	try:
		proc = subprocess.run(
			cmd.split(), check=True,
			stdout=subprocess.PIPE, stderr=subprocess.STDOUT,
			universal_newlines=True
		)
	except subprocess.CalledProcessError as e:
		logging.error(
			'command failed with status %d:\n%s',
			e.returncode, e.output.strip('\n')
		)
		raise
	else:
		logging.debug(
			'command completed successfully:\n%s',
			proc.stdout.strip('\n')
		)
		return proc


class Parser(argparse.ArgumentParser):
	"""
	ArgumentParser that parses files with 'key = value' lines.

	"""
	def __init__(self, *args, fromfile_prefix_chars='@', **kwargs):
		super().__init__(
			*args, fromfile_prefix_chars=fromfile_prefix_chars, **kwargs
		)

	def convert_arg_line_to_args(self, arg_line):
		# split each line on = and prepend prefix chars to first arg so it is
		# parsed as a long option
		args = [i.strip() for i in arg_line.split('=', maxsplit=1)]
		args[0] = 2*self.prefix_chars[0] + args[0]
		return args


parser = Parser(
	usage=''.join('\n  %(prog)s ' + i for i in [
		'[options] <results_file>',
		'checkpoint <checkpoint_file>',
		'-h | --help',
	]),
	description='''
Run relativistic heavy-ion collision events.

In the first form, run events according to the given options (below) and write
results to binary file <results_file>.

In the second form, run the event saved in <checkpoint_file>, previously
created by using the --checkpoint option and interrupting an event in progress.
''',
	formatter_class=argparse.RawDescriptionHelpFormatter
)


def parse_args():
	"""
	Parse command line arguments according to the parser usage info.  Return a
	tuple (args, ic) where `args` is a normal argparse.Namespace and `ic` is
	either None or an np.array of the checkpointed initial condition.

	First, check for the special "checkpoint" form, and if found, load and
	return the args and checkpoint initial condition from the specified file.
	If not, let the parser object handle everything.

	This is a little hacky but it works fine.  Truth is, argparse can't do
	exactly what I want here.  I suppose `docopt` might be a better option, but
	it's not worth the effort to rewrite everything.

	"""
	def usage():
		parser.print_usage(sys.stderr)
		sys.exit(2)

	if len(sys.argv) == 1:
		usage()

	return parser.parse_args()


parser.add_argument(
	'results', type=os.path.abspath,
	help=argparse.SUPPRESS
)
parser.add_argument(
	'--nevents', type=int, metavar='INT',
	help='number of events to run (default: run until interrupted)'
)
parser.add_argument(
	'--rankvar', metavar='VAR',
	help='environment variable containing process rank'
)
parser.add_argument(
	'--rankfmt', metavar='FMT',
	help='format string for rank integer'
)
parser.add_argument(
	'--tmpdir', type=os.path.abspath, metavar='PATH',
	help='temporary directory (default: {})'.format(tempfile.gettempdir())
)
parser.add_argument(
        '--startdir', type=os.path.abspath, metavar='PATH',
        help='start directory'
)
parser.add_argument(
	'--logfile', type=os.path.abspath, metavar='PATH',
	help='log file (default: stdout)'
)
parser.add_argument(
	'--loglevel', choices={'debug', 'info', 'warning', 'error', 'critical'},
	default='info',
	help='log level (default: %(default)s)'
)

class StopEvent(Exception):
	""" Raise to end an event early. """


def run_events(args, results_file):
	"""
	Run events as determined by user input:

		- Read options from `args`, as returned by `parser.parse_args()`.
		- Write results to binary file object `results_file`.

	Return True if at least one event completed successfully, otherwise False.

	"""
	results = np.empty((), dtype=result_dtype)

	Nevents = args.nevents
	def run_single_event(event_number):
		""" run a single jetscape event"""
		results.fill(0)
		start_h = datetime.datetime.now()
		h = run_cmd('TRENTO_FS_HYDRO')
		print(h.stdout)
		print(h.stderr)
		end_h = datetime.datetime.now()
		logging.info('TRENTO_FS_HYDRO finished in %s ', end_h - start_h)
		# hydro debugging files
		for rf, short in zip(['music_inv_reynolds_shear.txt',
					'music_inv_reynolds_bulk.txt'], ['iR-shear','iR-bulk']):
			if os.path.exists(rf):
				logging.info("{:s} exists".format(rf))
				subprocess.call("mv {:s} {:s}/{:s}-{:s}".format(
							rf,
							os.path.dirname(args.results),
							short,
							os.path.basename(args.results),
						),
				shell=True)
		if os.stat("surface.dat").st_size == 0:
			raise StopEvent('empty surface')
		else:
			subprocess.call("mv surface.dat input/", shell=True)
		# loop over delta-f
		for idf in range(3,4):
			subprocess.call("sed -i \"s/df_mode.*/df_mode = {:d}/g\" iS3D_parameters.dat".format(idf+1), shell=True)

			nparts = 0
			nos = 0
			nos_max = 500
			nos_min = 10
			npart_min = 100000
			subprocess.call("rm -f particles.dat", shell=True)

			while (nos < nos_max):
				nos += 1
				logging.info("Run smash # {:d}".format(nos))
				start_s = datetime.datetime.now()
				sample_burn = run_cmd("SAMPLER_AFTERBURNER")
				print(sample_burn.stdout)
				print(sample_burn.stderr)
				end_s = datetime.datetime.now()
				logging.info('SAMPLER_AFTERBURNER finished in %s ', end_s - start_s)
				subprocess.call("csplit --digits=2  --quiet --prefix=hadron_list final_smash_hadrons.dat \"/# JetScape module: SMASH/+1\" \"{*}\" ", shell=True)
				nparts += file_len("./hadron_list01")
				#subprocess.call("cat ./hadron_list00 >> particles_before_smash.dat", shell=True) #particles from iS3D note charge info is wrong
				subprocess.call("cat ./hadron_list01 >> particles.dat", shell=True) #particles after smash finished
				if ( nparts >= npart_min) and (nos >= nos_min):
					break

			logging.info("{:d} particles in {:d} oversamples".format(nparts, nos))

			# read final particle data (output from afterburner)
			start_o = datetime.datetime.now()
			with open("particles.dat", 'r') as f:
				particles = np.fromiter(
					(tuple(l.split()[2:]) for l in f if not l.startswith('#')),
					dtype=particle_dtype
				)

			
			#DEREK BEGIN
			"""
			# read particle data from iS3D
			with open("particles_before_smash.dat", 'r') as f:
				particles_before_smash = np.fromiter(
					(tuple(l.split()[2:]) for l in f if not l.startswith('#')),
					dtype=particle_dtype
				)

			
			#check for what range of rapidity the distribution dN/dy is flat
			
			y_bins_check_dN_dy = np.arange( -4.0, 4.0, 0.25 )
			y_before_smash = particles_before_smash['y']
			freq, bins = np.histogram( y_before_smash, y_bins_check_dN_dy )
			y_hist_before_smash = np.column_stack( (bins, freq[:-1] ) ) 
			np.savetxt('dN_dy_iS3D.dat', freq)
			subprocess.call("mv {:s} {:s}/{:s}-{:s}".format(
                                                        'dN_dy_iS3D.dat',
                                                        os.path.dirname(args.results),
                                                        'dN_dy_iS3D',
                                                        os.path.basename(args.results), ), shell=True)
			y_after_smash = particles['y']
			freq, bins = np.histogram( y_after_smash, y_bins_check_dN_dy ) 
			np.savetxt('dN_dy_smash.dat', freq)
			subprocess.call("mv {:s} {:s}/{:s}-{:s}".format(
                                                        'dN_dy_smash.dat',
                                                        os.path.dirname(args.results),
                                                        'dN_dy_smash',
                                                        os.path.basename(args.results), ), shell=True)
			"""
			#DEREK end

			logging.info('computing observables for delta_f = {:d}'.format(idf+1))

			# Get pT, phi, eta, y ...
			ET = particles['ET']
			pT = particles['pT']
			phi = particles['phi']
			eta = particles['eta']
			y = particles['y']
			charge = particles['charge']
			abs_pid = np.abs(particles['ID'])
			abs_eta = np.fabs(eta)
			abs_y = np.fabs(y)

			# charged particle cut
			charged = (charge != 0)

			# 1) calculate dNch/eta and dET/deta
			results['ALICE'][idf]['dNch_deta'] = \
							(charged & (abs_eta < .5)).sum() / (2*.5) / nos
			logging.info( "charged and abs_eta < 0.5 : " + str( (charged & (abs_eta < .5)).sum() / (2*.5) ) )
			results['ALICE'][idf]['dET_deta'] = \
							ET[abs_eta < .6].sum() / (2*.6) / nos

			# 2) calculate identified particle yield and mean_pT
			for name, pid in species:
				cut = (abs_pid == pid) & (abs_y < 0.5)
				N = cut.sum()
				results['ALICE'][idf]['dN_dy'][name] = N * 1. / nos
				results['ALICE'][idf]['mean_pT'][name] = 0 if N==0 else pT[cut].mean()

			# 3) calculate chagred particle pT fluctuation
			x = pT[charged & (abs_eta < .8) & (.15 < pT) & (pT < 2.)]
			N = x.size
			results['ALICE'][idf]['pT_fluct_chg']['N'] = N
			results['ALICE'][idf]['pT_fluct_chg']['sum_pT'] = 0. if N==0 else x.sum()
			results['ALICE'][idf]['pT_fluct_chg']['sum_pT2'] = 0. if N==0 else (x**2).sum()

			# 4) calculate identified particle pT fluctuation
			for name, pid in pi_K_p:
				x = pT[(abs_pid == pid) & (abs_y < 0.5)]
				N = x.size
				results['ALICE'][idf]['pT_fluct_pid'][name]['N'] = N
				results['ALICE'][idf]['pT_fluct_pid'][name]['sum_pT'] = 0. \
																if N == 0 else x.sum()
				results['ALICE'][idf]['pT_fluct_pid'][name]['sum_pT2'] = 0. \
																if N == 0 else (x**2).sum()

			# 5) pT-integrated Qvector with ALICE cut
			pTbins = np.array([[.2, 5.0]])
			etabins = np.array([[-0.8, 0.8]])
			info = calc_Qvector(pT[charged], phi[charged], eta[charged],
							 pTbins, etabins, Nharmonic)
			results['ALICE'][idf]['flow']['N'] = info['N'][0,0]
			results['ALICE'][idf]['flow']['Qn'] = info['Qn'][0,0,:]

			# 6) The magic Tmunu
			results['ALICE'][idf]['Tmunu'] = calc_Tmunu(ET, y, pT, phi,
										ycut=[-0.6, 0.6], pTcut=[0., 10.]) / nos

			results['ALICE'][idf]['Tmunu_chg'] = calc_Tmunu(ET[charged], y[charged],
										pT[charged], phi[charged],
										ycut=[-0.6, 0.6], pTcut=[0., 10.]) / nos

			# 7) Diff flow
			pTbins = np.array([[0,.2], [.2,.4], [.4,.6],[.6,.8],[.8,1.],
					[1.,1.2], [1.2,1.5], [1.5,2.], [2.,2.5], [2.5,3]])
			etabins = np.array([[-0.8, 0.8]])
			info = calc_Qvector(pT[charged], phi[charged], eta[charged],
							 pTbins, etabins, Nharmonic_diff)
			results['ALICE'][idf]['d_flow_chg']['N'] = info['N'][:,0]
			results['ALICE'][idf]['d_flow_chg']['Qn'] = info['Qn'][:,0,:]
			for name, pid in pi_K_p:
				cut = (abs_pid == pid)
				info = calc_Qvector(pT[cut], phi[cut], eta[cut],
							 pTbins, etabins, Nharmonic_diff)
				results['ALICE'][idf]['d_flow_pid'][name]['N'] = info['N'][:,0]
				results['ALICE'][idf]['d_flow_pid'][name]['Qn'] = info['Qn'][:,0,:]

			end_o = datetime.datetime.now()
			logging.info('Reading hadron list and computing observables finished in %s ', end_o - start_o)

	nfail = 0
	# run each initial condition event and save results to file
	for n in range(Nevents):
		logging.info('starting jetscape event %d', n)
		try:
			run_single_event(n)
		except StopEvent as e:
			logging.info('event stopped: %s', e)
		except Exception:
			logging.exception('event %d failed', n)
			nfail += 1
			if nfail/Nevents > .25:
				logging.critical('too many failures, stopping events')
				break
			logging.warning('continuing to next event')
			continue

		results_file.write(results.tobytes())
		logging.info('event %d completed successfully', n)

	return nfail


def main():
	args = parse_args()

	# starting fresh -> truncate output files
	filemode = 'w'

	# must handle rank first since it affects paths
	if args.rankvar:
		rank = os.getenv(args.rankvar)
		if rank is None:
			sys.exit('rank variable {} is not set'.format(args.rankvar))

		if args.rankfmt:
			rank = args.rankfmt.format(int(rank))

		# append rank to path arguments, e.g.:
		#   /path/to/output.log  ->  /path/to/output/<rank>.log
		for a in ['results', 'logfile']:
			value = getattr(args, a)
			if value is not None:
				root, ext = os.path.splitext(value)
				setattr(args, a, os.path.join(root, rank) + ext)


	os.makedirs(os.path.dirname(args.results), exist_ok=True)

	if args.logfile is None:
		logfile_kwargs = dict(stream=sys.stdout)
	else:
		logfile_kwargs = dict(filename=args.logfile, filemode=filemode)
		os.makedirs(os.path.dirname(args.logfile), exist_ok=True)

	logging.basicConfig(
		level=getattr(logging, args.loglevel.upper()),
		format='[%(levelname)s@%(relativeCreated)d] %(message)s',
		**logfile_kwargs
	)
	logging.captureWarnings(True)

	start = datetime.datetime.now()

	logging.info('started at %s', start)
	logging.info('arguments: %r', args)


	# translate SIGTERM to KeyboardInterrupt
	signal.signal(signal.SIGTERM, signal.default_int_handler)
	logging.debug('set SIGTERM handler')
	prefix=str(args.startdir)
	logging.info('starting dir: {:s}'.format(prefix))
	with \
			open(args.results, filemode + 'b') as results_file, \
			tempfile.TemporaryDirectory(
				prefix='jetscape-', dir=args.tmpdir) as workdir:
		os.chdir(workdir)
		os.mkdir("input/")
		os.system( 'ln -s {:s}/jetscape_init.xml jetscape_init.xml'.format(prefix)  )
		os.system( 'ln -s {:s}/freestream_input freestream_input'.format(prefix)  )
		os.system( 'ln -s {:s}/music_input music_input'.format(prefix) )
		os.system( 'ln -s {:s}/iS3D_parameters.dat iS3D_parameters.dat'.format(prefix)  )
		os.system( 'ln -s {:s}/PDG PDG'.format(prefix)  )
		os.system( 'ln -s {:s}/EOS EOS'.format(prefix)  )
		os.system( 'ln -s {:s}/tables tables'.format(prefix)  )
		os.system( 'ln -s {:s}/deltaf_coefficients deltaf_coefficients'.format(prefix)  )
		os.system( 'ln -s {:s}/smash_input smash_input'.format(prefix) )


		logging.info('working directory: %s', workdir)

		try:
			status = run_events(args, results_file) #run_events returns the number of failed events
		except KeyboardInterrupt:
			# after catching the initial SIGTERM or interrupt, ignore them
			# during shutdown -- this ensures everything will exit gracefully
			# in case of additional signals (short of SIGKILL)
			signal.signal(signal.SIGTERM, signal.SIG_IGN)
			signal.signal(signal.SIGINT, signal.SIG_IGN)
			status = True
			logging.info(
				'interrupt or signal at %s, cleaning up...',
				datetime.datetime.now()
			)
			if args.checkpoint is not None:
				logging.info(
					'current event saved in checkpoint file %s',
					args.checkpoint
				)

	end = datetime.datetime.now()
	logging.info('finished at %s, %s elapsed', end, end - start)

        #if events fail or keyboard interrupt status > 0
	if status:
		sys.exit(1)


if __name__ == "__main__":
	main()
